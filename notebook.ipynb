{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from functions.config import get_config\n",
    "from functions.models.pose_hrnet import PoseHighResolutionNet\n",
    "\n",
    "vid_path = 'files\\\\videos'\n",
    "img_path = 'files\\\\images'\n",
    "dataset_dir = 'finetune_dataset'\n",
    "CTX = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "box_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "box_model.to(CTX)\n",
    "box_model.eval()\n",
    "\n",
    "# initializing model\n",
    "cfg = get_config(weights_path='pose_hrnet_w32_384x288.pth')\n",
    "pose_model = PoseHighResolutionNet(cfg)\n",
    "pose_model.init_weights(cfg.MODEL.PRETRAINED)\n",
    "pose_model.to(CTX)\n",
    "pose_model.eval()\n",
    "vid_filepath = os.path.join(vid_path, os.listdir(vid_path)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8373fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.processing.boxes import retrieve_reps, analyze_boxes\n",
    "from functions.visualization import visualize_reps\n",
    "boxes, divisor, total_frames = analyze_boxes(vid_filepath, box_model)\n",
    "boxes_new, indices, divisor, total_frames = retrieve_reps(boxes, divisor, total_frames)\n",
    "visualize_reps(boxes, divisor, total_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64156a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.processing.keypoints import get_start_end_indices, get_squat_reps, get_even_indices\n",
    "squat_reps = get_squat_reps(indices, boxes_new, vid_filepath, box_model, pose_model)\n",
    "start, end = get_start_end_indices(indices)\n",
    "even_indices = get_even_indices(start, end)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from functions.models.hot_mixste import Model\n",
    "from functions.processing.pose3d import create_adaptive_args, camera_to_world, normalize_screen_coordinates, pad_sequence_to_length\n",
    "from functions.visualization import show2Dpose, show3Dpose, showimage\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "vidfile = os.listdir(vid_path)[1]\n",
    "args = create_adaptive_args()\n",
    "fix_z = True\n",
    "output_dir = 'results/' + vidfile.split('.')[0] + '/'\n",
    "vid_filepath = os.path.join(vid_path, vidfile)\n",
    "hotmodel = Model(args)\n",
    "hotmodel.to(CTX)\n",
    "\n",
    "model_dict = hotmodel.state_dict()\n",
    "model_path = sorted(glob.glob(os.path.join(args.checkpoint, '*.pth')))[0]\n",
    "pre_dict = torch.load(model_path)\n",
    "model_dict = hotmodel.state_dict()\n",
    "state_dict = {k: v for k, v in pre_dict.items() if k in model_dict.keys()}\n",
    "model_dict.update(state_dict)\n",
    "hotmodel.load_state_dict(model_dict)\n",
    "\n",
    "hotmodel.eval()\n",
    "\n",
    "cap = cv2.VideoCapture(vid_filepath)\n",
    "video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_index = 0\n",
    "for rep in squat_reps.keys():\n",
    "    print('\\nProcessing rep: ' + str(rep) + ' / ' + str(len(squat_reps.keys())))\n",
    "    # make directories\n",
    "    output_dir_2D = output_dir + 'rep_' + str(rep) + '/pose2D/'\n",
    "    os.makedirs(output_dir_2D, exist_ok=True)\n",
    "    output_dir_3D = output_dir + 'rep_' + str(rep) + '/pose3D/'\n",
    "    os.makedirs(output_dir_3D, exist_ok=True)\n",
    "    os.makedirs(output_dir + 'rep_' + str(rep) + '/' + 'output_3D/', exist_ok=True)\n",
    "\n",
    "    keypoints = np.array(squat_reps[rep]['Keypoints'])\n",
    "    rep_length = keypoints.shape[0]\n",
    "    ret, img = cap.read()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_size = args.image_size\n",
    "    img = cv2.resize(img, (img_size[0], img_size[1]))\n",
    "    print('\\nGenerating 3D pose...')\n",
    "    frame_sum = 0\n",
    "    input_2D_no = keypoints.copy()\n",
    "\n",
    "    joints_left =  [4, 5, 6, 11, 12, 13]\n",
    "    joints_right = [1, 2, 3, 14, 15, 16]\n",
    "\n",
    "    input_2D = normalize_screen_coordinates(input_2D_no, w=img_size[0], h=img_size[1])  \n",
    "    input_2D_aug = copy.deepcopy(input_2D)\n",
    "    input_2D_aug[ :, :, 0] *= -1\n",
    "    input_2D_aug[ :, joints_left + joints_right] = input_2D_aug[ :, joints_right + joints_left]\n",
    "    input_2D = np.concatenate((np.expand_dims(input_2D, axis=0), np.expand_dims(input_2D_aug, axis=0)), 0)    \n",
    "    input_2D = input_2D[np.newaxis, :, :, :, :]\n",
    "    input_2D = torch.from_numpy(input_2D.astype('float32')).cuda()\n",
    "    \n",
    "    input_2D = pad_sequence_to_length(input_2D, target_length=243)\n",
    "    N = input_2D.size(0)\n",
    "\n",
    "    ## estimation\n",
    "    with torch.no_grad():\n",
    "        output_3D_non_flip = hotmodel(input_2D[:, 0])\n",
    "        output_3D_flip     = hotmodel(input_2D[:, 1])\n",
    "\n",
    "    output_3D_flip[:, :, :, 0] *= -1\n",
    "    output_3D_flip[:, :, joints_left + joints_right, :] = output_3D_flip[:, :, joints_right + joints_left, :] \n",
    "\n",
    "    output_3D = (output_3D_non_flip + output_3D_flip) / 2\n",
    "\n",
    "    output_3D[:, :, 0, :] = 0\n",
    "    post_out = output_3D[0].cpu().detach().numpy()\n",
    "    output_3D_all = post_out.copy()\n",
    "    rot =  [0.1407056450843811, -0.1500701755285263, -0.755240797996521, 0.6223280429840088]\n",
    "    rot = np.array(rot, dtype='float32')\n",
    "    post_out = camera_to_world(post_out, R=rot, t=0)\n",
    "    if frame_index == 0:\n",
    "        for i in range(0, start[0]):\n",
    "            ret, img = cap.read()\n",
    "        frame_index = start[0]\n",
    "    while frame_index < start[rep-1]:\n",
    "        frame_index += 1\n",
    "        ret, img = cap.read()\n",
    "    while frame_index < end[rep-1]:\n",
    "        ret, img = cap.read()\n",
    "        if frame_index % 2 == 0 or not ret:\n",
    "            frame_index += 1\n",
    "            continue\n",
    "        print('Processing frame: ' + str(frame_index))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (img_size[0], img_size[1]))\n",
    "        image = show2Dpose(input_2D_no[(frame_index - start[rep-1])//2], copy.deepcopy(img))\n",
    "        cv2.imwrite(output_dir_2D + str(('%04d'% (frame_index - start[rep-1]))) + '_2D.png', image)\n",
    "        frame_index += 1\n",
    "\n",
    "        ## 3D\n",
    "        fig = plt.figure(figsize=(9.6, 5.4))\n",
    "        gs = gridspec.GridSpec(1, 1)\n",
    "        gs.update(wspace=-0.00, hspace=0.05) \n",
    "        ax = plt.subplot(gs[0], projection='3d')\n",
    "        post_out = output_3D_all[frame_index - start[rep-1]]\n",
    "        show3Dpose(post_out, ax, fix_z)\n",
    "        plt.savefig(output_dir_3D + str(('%04d'% (frame_index - start[rep-1]))) + '_3D.png', dpi=200, format='png', bbox_inches = 'tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "    ## save 3D keypoints\n",
    "    output_npz = output_dir + 'rep_' + str(rep)  + '/output_3D/' + 'output_3D.npz'\n",
    "    np.savez_compressed(output_npz, reconstruction=output_3D_all)\n",
    "\n",
    "    print('Generating 3D pose successfully!')\n",
    "\n",
    "    ## all\n",
    "    image_dir = 'results/' \n",
    "    image_2d_dir = sorted(glob.glob(os.path.join(output_dir_2D, '*.png')))\n",
    "    image_3d_dir = sorted(glob.glob(os.path.join(output_dir_3D, '*.png')))\n",
    "\n",
    "    print('\\nGenerating demo...')\n",
    "    for i in tqdm(range(len(image_3d_dir))):\n",
    "        image_3d = plt.imread(image_3d_dir[i])\n",
    "\n",
    "        edge = 0\n",
    "        image_3d = image_3d[edge:image_3d.shape[0] - edge, edge:image_3d.shape[1] - edge]\n",
    "        image_2d = plt.imread(image_2d_dir[i])\n",
    "        ## show\n",
    "        font_size = 12\n",
    "        fig = plt.figure(figsize=(9.6, 5.4))\n",
    "        ax = plt.subplot(121)\n",
    "        showimage(ax, image_2d)\n",
    "        ax.set_title(\"Input\", fontsize = font_size)\n",
    "\n",
    "        ax = plt.subplot(122)\n",
    "        showimage(ax, image_3d)\n",
    "        ax.set_title(\"Reconstruction\", fontsize = font_size)\n",
    "\n",
    "        ## save\n",
    "        output_dir_pose = output_dir + 'rep_' + str(rep) + '/pose/'\n",
    "        os.makedirs(output_dir_pose, exist_ok=True)\n",
    "        plt.savefig(output_dir_pose + str(('%04d'% i)) + '_pose.png', dpi=200, bbox_inches = 'tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "437ca791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video for rep 1 saved as results/VID_20211224_120346353 - Trim/rep_1/output.mp4\n",
      "Video for rep 2 saved as results/VID_20211224_120346353 - Trim/rep_2/output.mp4\n",
      "Video for rep 3 saved as results/VID_20211224_120346353 - Trim/rep_3/output.mp4\n",
      "Video for rep 4 saved as results/VID_20211224_120346353 - Trim/rep_4/output.mp4\n",
      "Video for rep 5 saved as results/VID_20211224_120346353 - Trim/rep_5/output.mp4\n"
     ]
    }
   ],
   "source": [
    "# turn images into video\n",
    "import cv2\n",
    "def images_to_video(image_folder, video_name, fps=30):\n",
    "    images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "    images.sort()  # Ensure the images are sorted\n",
    "    frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    for image in images:\n",
    "        video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "# Create video for each rep\n",
    "for rep in squat_reps.keys():\n",
    "    output_dir_pose = output_dir + 'rep_' + str(rep) + '/pose/'\n",
    "    video_name = output_dir + 'rep_' + str(rep) + '/output.mp4'\n",
    "    images_to_video(output_dir_pose, video_name, fps=30)\n",
    "    print(f\"Video for rep {rep} saved as {video_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = [[16,14],[14,12],[17,15],[15,13],[12,13],[6,12],[7,13], [6,7],[6,8],\n",
    "        [7,9],[8,10],[9,11],[2,3],[1,2],[1,3],[2,4],[3,5],[4,6],[5,7]]\n",
    "skel = [[coord-1 for coord in pair] for pair in skeleton]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "skeleton = [[16,14],[14,12],[17,15],[15,13],[12,13],[6,12],[7,13], [6,7],[6,8],\n",
    "        [7,9],[8,10],[9,11],[2,3],[1,2],[1,3],[2,4],[3,5],[4,6],[5,7]]\n",
    "skel = [[coord-1 for coord in pair] for pair in skeleton]\n",
    "def plot_3d_keypoints_with_skeleton(keypoints, skeleton, frame_index=0):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot each joint\n",
    "    for joint in keypoints[frame_index]:\n",
    "        ax.scatter(joint[0], joint[1], joint[2], marker='o')\n",
    "    \n",
    "    # Plot skeleton\n",
    "    for pair in skeleton:\n",
    "        joint1 = keypoints[frame_index][pair[0]]\n",
    "        joint2 = keypoints[frame_index][pair[1]]\n",
    "        ax.plot([joint1[0], joint2[0]], [joint1[1], joint2[1]], [joint1[2], joint2[2]], 'r-')\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title(f'3D Keypoints with Skeleton - Frame {frame_index}')\n",
    "    \n",
    "    plt.show()\n",
    "# Example usage\n",
    "for frame_index in range(0, len(output_3d_all), 10):  # Adjust step for fewer frames\n",
    "    plot_3d_keypoints_with_skeleton(output_3d_all, skel, frame_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd35191",
   "metadata": {},
   "source": [
    "1. Folders of videos :)\n",
    "2. do random sampling on all videos, if confidence abysmally low or bbox unfindable, bin video as too low quality\n",
    "3. Pick single video :)\n",
    "4. Preprocess into separate frames :)\n",
    "5. Calculate bbox for every frame :)\n",
    "6. Decide if BBOX is recognizable in video -> bin videos where bbox is not found for at least 10% (?) of frames :)\n",
    "7. Calculate HRNET for first 15 frames -> if confidence < 0.5 bin the video\n",
    "8. Calculate HRNET for all frames\n",
    "9. Run 3d pose estimation of all frames\n",
    "10. Retrieve 3d frame position of lower body joints\n",
    "11. Observe changes in joints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
